{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\r0776327\\AppData\\Local\\miniconda3\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper_functions.model_builder import LSTM\n",
    "from helper_functions.data_setup import TimeseriesDataset\n",
    "from helper_functions.engine import train\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, split\n",
    "import pandas as pd\n",
    "from copy import deepcopy as dc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                  Date  Price  FR Generation  FR Load\n",
       " 0  2018-01-01 00:00:00   4.74        53625.0  56250.0\n",
       " 1  2018-01-01 01:00:00   3.66        52398.0  54300.0\n",
       " 2  2018-01-01 02:00:00   1.26        51825.0  53600.0\n",
       " 3  2018-01-01 03:00:00 -20.10        50729.0  50000.0\n",
       " 4  2018-01-01 04:00:00 -31.82        50719.0  47100.0,\n",
       " (43824, 4))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Datasets/Dataframes_one_ex_var/ModelBenchmark_dataframe.csv\")\n",
    "df.head(), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date  Price\n",
       "0  2018-01-01 00:00:00   4.74\n",
       "1  2018-01-01 01:00:00   3.66"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Date\", \"Price\"]]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Price\n",
       "Date                      \n",
       "2018-01-01 00:00:00   4.74\n",
       "2018-01-01 01:00:00   3.66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Date = pd.to_datetime(df.Date)\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "df = df.astype(\"float32\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test period\n",
    "start_test_period = df.index.get_loc(\"2022-01-01\").start\n",
    "end_test_period = df.index.get_loc(\"2022-12-31\").stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461, 24, 1)\n",
      "4.74 82.02\n",
      "(365, 24, 1)\n",
      "67.07 -4.39\n"
     ]
    }
   ],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(df, start_test_period:int, end_test_period):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = df[0:start_test_period], df[start_test_period:end_test_period]\n",
    "\t# restructure into windows of daily data\n",
    "\ttrain = array(split(train, len(train)/24))\n",
    "\ttest = array(split(test, len(test)/24))\n",
    "\treturn train, test\n",
    "\n",
    "# load the new file\n",
    "train_set, test_set = split_dataset(df.values, start_test_period, end_test_period)\n",
    "# validate train data\n",
    "print(train_set.shape) #1461 = 365*3 + 366\n",
    "print(train_set[0, 0, 0], train_set[-1, -1, 0])\n",
    "# validate test\n",
    "print(test_set.shape)\n",
    "print(test_set[0, 0, 0], test_set[-1, -1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34873, 168, 1), (34873, 24))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=24):\n",
    "\t\"\"\"\n",
    "\ttrain = list of days (history to be considered)\n",
    "\tn_input = number of time steps to use as inputs\n",
    "\tn_out = forecast horizon (here always 24 hour ahead)\n",
    "\t\"\"\"\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "X_train, y_train = to_supervised(train_set, n_input=24*7)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8569, 168, 1), (8569, 24))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = to_supervised(test_set, n_input=24*7)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([34873, 168, 1]),\n",
       " torch.Size([34873, 24]),\n",
       " torch.Size([8569, 168, 1]),\n",
       " torch.Size([8569, 24]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeseriesDataset(X_train, y_train)\n",
    "test_dataset = TimeseriesDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 168, 1]) torch.Size([16, 24])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for _, batch in enumerate(train_loader):\n",
    "    x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "    print(x_batch.shape, y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 4, batch_first=True)\n",
       "  (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(1, 4, 1)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to training mode and then\n",
    "  runs through all of the required training steps (forward\n",
    "  pass, loss calculation, optimizer step).\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "  \"\"\"\n",
    "  # Put model in train mode\n",
    "  model.train(True)\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      y_pred = model(X)\n",
    "\n",
    "      # 2. Calculate  and accumulate loss\n",
    "      loss = loss_fn(y_pred, y)\n",
    "      train_loss += loss.item() \n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate and accumulate accuracy metric across all batches\n",
    "      # avg_loss_accross_batches = train_loss / 100\n",
    "      train_acc += mae(y_pred.detach(), y)\n",
    "      # y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      # train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "  a forward pass on a testing dataset.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "  \"\"\"\n",
    "  # Put model in eval mode\n",
    "  model.eval() \n",
    "\n",
    "  # Setup test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.inference_mode():\n",
    "      # Loop through DataLoader batches\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "          # Send data to target device\n",
    "          X, y = X.to(device), y.to(device)\n",
    "\n",
    "          # 1. Forward pass\n",
    "          test_pred = model(X)\n",
    "\n",
    "          # 2. Calculate and accumulate loss\n",
    "          loss = loss_fn(test_pred, y)\n",
    "          test_loss += loss.item()          \n",
    "          \n",
    "          # Calculate and accumulate accuracy\n",
    "          test_acc += mae(test_pred.detach(), y)\n",
    "          \n",
    "          # test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "  \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "  Passes a target PyTorch models through train_step() and test_step()\n",
    "  functions for a number of epochs, training and testing the model\n",
    "  in the same epoch loop.\n",
    "\n",
    "  Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "                  train_acc: [...],\n",
    "                  test_loss: [...],\n",
    "                  test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "                 {train_loss: [2.0616, 1.0537],\n",
    "                  train_acc: [0.3945, 0.3945],\n",
    "                  test_loss: [1.2641, 1.5706],\n",
    "                  test_acc: [0.3400, 0.2973]} \n",
    "  \"\"\"\n",
    "  # Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "  }\n",
    "\n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "      train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "      test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "      # Print out what's happening\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_MAE: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_MAE: {test_acc:.4f}\"\n",
    "      )\n",
    "\n",
    "      # Update results dictionary\n",
    "      results[\"train_loss\"].append(train_loss)\n",
    "      results[\"train_acc\"].append(train_acc)\n",
    "      results[\"test_loss\"].append(test_loss)\n",
    "      results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  # Return the filled results at the end of the epochs\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2797],\n",
      "        [-0.2481],\n",
      "        [-0.2236],\n",
      "        [-0.2807],\n",
      "        [-0.2930],\n",
      "        [-0.2888],\n",
      "        [-0.2925],\n",
      "        [-0.2958],\n",
      "        [-0.2947],\n",
      "        [-0.3011],\n",
      "        [-0.3016],\n",
      "        [-0.3621],\n",
      "        [-0.2681],\n",
      "        [-0.2899],\n",
      "        [-0.2935],\n",
      "        [-0.3168]], grad_fn=<AddmmBackward0>) tensor([[ 5.6150e+01,  5.5000e+01,  6.4310e+01,  8.1700e+01,  9.1950e+01,\n",
      "          1.0004e+02,  1.1270e+02,  9.2900e+01,  9.4480e+01,  8.4680e+01,\n",
      "          7.9470e+01,  5.5060e+01,  4.3920e+01,  2.2050e+01,  1.5000e+01,\n",
      "          4.1090e+01,  5.8290e+01,  7.7540e+01,  8.8810e+01,  8.5420e+01,\n",
      "          7.6640e+01,  6.0610e+01,  6.1380e+01,  5.3880e+01],\n",
      "        [ 7.8500e+01,  7.0000e+01,  8.4430e+01,  6.7690e+01,  5.3340e+01,\n",
      "          5.2680e+01,  5.2810e+01,  5.0470e+01,  5.1330e+01,  5.3040e+01,\n",
      "          5.7260e+01,  5.9800e+01,  6.6920e+01,  6.7710e+01,  6.8000e+01,\n",
      "          6.7010e+01,  6.2530e+01,  6.0000e+01,  6.0000e+01,  6.2780e+01,\n",
      "          6.9920e+01,  7.1990e+01,  6.8950e+01,  6.2660e+01],\n",
      "        [ 1.7906e+02,  1.5700e+02,  1.4793e+02,  1.4006e+02,  1.3530e+02,\n",
      "          1.2554e+02,  1.3072e+02,  1.4163e+02,  1.5093e+02,  1.5909e+02,\n",
      "          1.8461e+02,  2.0332e+02,  1.9822e+02,  1.6091e+02,  1.4715e+02,\n",
      "          1.3118e+02,  1.2223e+02,  1.2159e+02,  1.1383e+02,  1.0784e+02,\n",
      "          1.1003e+02,  1.2203e+02,  1.4990e+02,  1.6792e+02],\n",
      "        [ 5.5000e+01,  4.8380e+01,  4.6910e+01,  4.7880e+01,  4.4600e+01,\n",
      "          4.3320e+01,  4.7500e+01,  5.5270e+01,  6.3160e+01,  7.0300e+01,\n",
      "          7.0300e+01,  6.6470e+01,  6.2850e+01,  5.9020e+01,  5.8010e+01,\n",
      "          5.7960e+01,  5.8040e+01,  6.0350e+01,  6.3060e+01,  6.5790e+01,\n",
      "          6.5890e+01,  6.1770e+01,  5.2930e+01,  5.2830e+01],\n",
      "        [ 3.6740e+01,  5.2730e+01,  6.1560e+01,  5.8210e+01,  6.2480e+01,\n",
      "          5.8810e+01,  5.0990e+01,  5.4870e+01,  5.3480e+01,  4.9730e+01,\n",
      "          4.8530e+01,  4.2640e+01,  5.5700e+01,  5.8180e+01,  5.8060e+01,\n",
      "          4.9960e+01,  4.7470e+01,  4.6540e+01,  4.0620e+01,  3.5580e+01,\n",
      "          2.9660e+01,  2.6010e+01,  1.9570e+01,  1.6450e+01],\n",
      "        [ 5.5060e+01,  5.5920e+01,  5.5930e+01,  4.8100e+01,  4.4920e+01,\n",
      "          4.1640e+01,  3.9410e+01,  3.4080e+01,  3.2200e+01,  3.2560e+01,\n",
      "          3.1080e+01,  2.8860e+01,  2.8920e+01,  3.5210e+01,  4.6270e+01,\n",
      "          4.7630e+01,  4.5680e+01,  4.3610e+01,  4.0170e+01,  3.7330e+01,\n",
      "          3.4040e+01,  3.2900e+01,  3.2540e+01,  3.3210e+01],\n",
      "        [ 3.9610e+01,  4.0390e+01,  4.4790e+01,  5.2970e+01,  5.7710e+01,\n",
      "          5.9350e+01,  5.7810e+01,  5.8440e+01,  5.5000e+01,  4.5500e+01,\n",
      "          4.2250e+01,  3.9650e+01,  3.8480e+01,  3.6100e+01,  3.5690e+01,\n",
      "          3.8040e+01,  3.8960e+01,  4.6050e+01,  5.7350e+01,  5.7750e+01,\n",
      "          4.9260e+01,  4.2120e+01,  3.8310e+01,  4.0510e+01],\n",
      "        [ 2.8920e+01,  3.5210e+01,  4.6270e+01,  4.7630e+01,  4.5680e+01,\n",
      "          4.3610e+01,  4.0170e+01,  3.7330e+01,  3.4040e+01,  3.2900e+01,\n",
      "          3.2540e+01,  3.3210e+01,  4.2930e+01,  4.5630e+01,  4.1030e+01,\n",
      "          3.3890e+01,  3.1260e+01,  3.1300e+01,  2.9700e+01,  2.8060e+01,\n",
      "          2.7670e+01,  2.8330e+01,  2.6720e+01,  2.6950e+01],\n",
      "        [ 4.4060e+01,  4.1090e+01,  3.8170e+01,  3.8040e+01,  3.7060e+01,\n",
      "          3.5150e+01,  3.4750e+01,  3.4240e+01,  3.4740e+01,  3.6710e+01,\n",
      "          4.0340e+01,  4.7000e+01,  4.3350e+01,  4.0340e+01,  4.3880e+01,\n",
      "          3.6660e+01,  3.5910e+01,  3.3540e+01,  3.2980e+01,  3.1480e+01,\n",
      "          2.9240e+01,  3.0160e+01,  3.9010e+01,  4.6720e+01],\n",
      "        [ 1.8980e+01,  2.3670e+01,  2.8300e+01,  3.0870e+01,  3.3000e+01,\n",
      "          3.4770e+01,  3.4940e+01,  3.1580e+01,  3.0110e+01,  2.5070e+01,\n",
      "          2.2150e+01,  2.0260e+01,  1.6200e+01,  1.2530e+01,  1.0980e+01,\n",
      "          1.1250e+01,  1.4000e+01,  1.3680e+01,  1.5600e+01,  1.7000e+01,\n",
      "          1.8620e+01,  1.3080e+01,  8.2600e+00,  8.4000e+00],\n",
      "        [ 1.5020e+01,  2.1970e+01,  3.0440e+01,  3.3910e+01,  3.9450e+01,\n",
      "          3.0830e+01,  2.7110e+01,  3.0720e+01,  2.7850e+01,  2.2000e+01,\n",
      "          1.9150e+01,  9.2900e+00,  7.5400e+00,  1.1370e+01,  1.6840e+01,\n",
      "          3.0340e+01,  3.9390e+01,  4.5430e+01,  5.4640e+01,  6.0350e+01,\n",
      "          5.2570e+01,  5.0530e+01,  4.9950e+01,  4.7820e+01],\n",
      "        [ 3.4900e+01,  3.3100e+01,  3.3050e+01,  3.4600e+01,  3.6640e+01,\n",
      "          3.8860e+01,  3.8430e+01,  3.8390e+01,  3.5510e+01,  3.3610e+01,\n",
      "          3.2030e+01,  2.9790e+01,  2.8910e+01,  3.0090e+01,  3.4560e+01,\n",
      "          4.1550e+01,  4.1640e+01,  4.4990e+01,  4.2250e+01,  3.9100e+01,\n",
      "          4.0350e+01,  3.7930e+01,  3.9150e+01,  3.6050e+01],\n",
      "        [ 5.2000e+01,  5.2000e+01,  5.2000e+01,  5.3300e+01,  5.5000e+01,\n",
      "          6.9880e+01,  8.4980e+01,  1.0000e+02,  1.0173e+02,  9.3920e+01,\n",
      "          8.6910e+01,  7.5000e+01,  6.9000e+01,  6.3100e+01,  6.2690e+01,\n",
      "          6.2390e+01,  6.6620e+01,  6.5080e+01,  8.1010e+01,  8.7610e+01,\n",
      "          8.3810e+01,  5.9140e+01,  5.6320e+01,  6.0420e+01],\n",
      "        [ 4.0360e+01,  3.7530e+01,  3.3980e+01,  3.2600e+01,  2.6990e+01,\n",
      "          2.7850e+01,  3.3620e+01,  8.3700e+01,  7.9900e+01,  6.0510e+01,\n",
      "          6.2610e+01,  5.3090e+01,  4.6050e+01,  4.2850e+01,  4.3210e+01,\n",
      "          4.0400e+01,  3.5270e+01,  2.8380e+01,  3.3400e+01,  4.1090e+01,\n",
      "          3.4290e+01,  3.3000e+01,  3.6650e+01,  3.6180e+01],\n",
      "        [ 3.7170e+01,  3.9020e+01,  4.7630e+01,  5.4580e+01,  5.6170e+01,\n",
      "          5.3070e+01,  5.0390e+01,  4.6930e+01,  4.3940e+01,  3.9900e+01,\n",
      "          3.9020e+01,  3.7510e+01,  3.7380e+01,  4.2700e+01,  4.8900e+01,\n",
      "          5.0480e+01,  5.0630e+01,  5.1450e+01,  5.1220e+01,  4.8470e+01,\n",
      "          4.4060e+01,  4.0000e+01,  3.7940e+01,  3.4100e+01],\n",
      "        [ 1.6720e+01,  1.5530e+01, -2.4100e+00, -3.2000e+01, -1.9000e+01,\n",
      "         -3.9210e+01, -4.0000e+01, -4.4510e+01, -1.2040e+01,  7.9800e+00,\n",
      "          1.2200e+01,  1.2950e+01,  1.3410e+01,  1.4560e+01,  1.3990e+01,\n",
      "          3.0000e-02, -1.4200e+00, -6.7400e+00, -1.7020e+01, -1.8570e+01,\n",
      "         -2.1770e+01, -2.8180e+01, -2.4980e+01, -2.7440e+01]])\n",
      "torch.Size([16, 168, 1]) torch.Size([16, 1]) torch.Size([16, 24])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Start training with help from engine.py\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\r0776327\\OneDrive\\Documents\\KUL\\HIR 5\\Thesis\\DL_EPF\\helper_functions\\engine.py:172\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m--> 172\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    178\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[0;32m    179\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m    180\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\r0776327\\OneDrive\\Documents\\KUL\\HIR 5\\Thesis\\DL_EPF\\helper_functions\\engine.py:64\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, y_pred\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Calculate and accumulate accuracy metric across all batches\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# avg_loss_accross_batches = train_loss / 100\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     train_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmae\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# train_acc += (y_pred_class == y).sum().item()/len(y_pred)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Adjust metrics to get average loss and accuracy per batch \u001b[39;00m\n\u001b[0;32m     69\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "File \u001b[1;32mc:\\Users\\r0776327\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\r0776327\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:204\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    141\u001b[0m     {\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m ):\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    208\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\r0776327\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:110\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true and y_pred have different number of output (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    112\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    113\u001b[0m         )\n\u001b[0;32m    114\u001b[0m     )\n\u001b[0;32m    116\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    117\u001b[0m allowed_multioutput_str \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance_weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=24)"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training with help from engine.py\n",
    "train(model=model,\n",
    "             train_dataloader=train_loader,\n",
    "             test_dataloader=test_loader,\n",
    "             loss_fn=loss_function,\n",
    "             optimizer=optimizer,\n",
    "             epochs=num_epochs,\n",
    "             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
